{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from gensim.models import KeyedVectors, FastText\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import operator\n",
    "\n",
    "# import natural language toolkit\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation #, strip_numbers\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.library.postgresql import PostgresQL\n",
    "from modules.library.word_models import WordModels\n",
    "from modules.library.document_similarity import DocumentSimilarity\n",
    "from modules.library.document_models import DocumentModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_pickle = True\n",
    "dump_to_pickle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_from_pickle:\n",
    "    with open(\"document_analysis_variables.pkl\", \"rb\") as f:\n",
    "        e = pickle.load(f)\n",
    "    documents = e[0]\n",
    "    sample_model = e[1]\n",
    "    document_texts = e[2]\n",
    "    document_model = e[3]\n",
    "    embedding = e[4]\n",
    "    u = e[5]\n",
    "    s = e[6]\n",
    "    vh = e[7]\n",
    "    document_embedding = e[8]\n",
    "    ds = e[9]\n",
    "    list_of_neighbors = e[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the database\n",
    "\n",
    "Using the module PostgresQL we will load the documents from our database and store them in a list 'documents'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "pg = PostgresQL() \n",
    "pg.connect(database=\"eurlex_environment_only\", user=\"postgres\", password=\"dbpass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_from_pickle:\n",
    "    documents = pg.execute(\"\"\"\n",
    "        SELECT * FROM documents;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddnig model\n",
    "\n",
    "In this section we will show how to use the module WordModels to load or train a word embedding model. In order to avoid runnig a time consuming commands, we will comment them out and use pickle instead to store and access already loaded or trained models. \n",
    "\n",
    "## Training a word embedding model\n",
    "\n",
    "In case we want to use pre-trained models to train another model on our data, we can use a method in module WordModels called 'train' in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"dogs like chasing cats\", \"cats like chasing mice\", \"mice eat cheese\", \"cheese has holes\", \"earth flat\", \"nobody knows working\"]\n",
    "\n",
    "if not load_from_pickle:\n",
    "    sample_model = WordModels()\n",
    "    sample_model.train(texts, size=300, window=1, min_count=1, epochs=10) # texts is a list of stripped strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['d', 'o', 'g', 's', ' ', 'l', 'i', 'k', 'e', 'c', 'h', 'a', 'n', 't', 'm', 'r', 'f', 'b', 'y', 'w'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.get_embedding().vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modules.library.word_models.WordModels"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.FastTextKeyedVectors"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample_model.get_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a word embedding model\n",
    "\n",
    "In case model has already been trained either by the user or has been provided from another source, module WordModels enables us to load it and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_from_pickle:\n",
    "    wiki_en_path = '../data/fasttext/wiki.en.align.vec'\n",
    "    wiki_en_model = WordModels()\n",
    "    wiki_en_model.load(wiki_en_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_from_pickle:\n",
    "    with open(\"wwe_word_models.pkl\", \"rb\") as f:\n",
    "        wiki_en_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dump_to_pickle:\n",
    "    with open(\"wwe_word_models.pkl\", \"wb\") as f:\n",
    "        pickle.dump(wiki_en_model, f, protocol=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not load_from_pickle:\n",
    "#     wiki_bin_path ='../data/fasttext/wiki.en.bin' \n",
    "#     wiki_bin_model = WordModels()\n",
    "#     wiki_bin_model.load(wiki_bin_path, model_type='fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_law = '../data/Law2Vec/Law2Vec.200d.txt'\n",
    "# law_model = WordModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# law_model.load(path_law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"law_we.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(law_model, f, protocol = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"law_we.pkl\", \"rb\") as f:\n",
    "#     law_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding documents\n",
    "\n",
    "We have some documents saved in a list 'documents'. Let's embed them using DocumentModels module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_from_pickle:\n",
    "    document_texts = [doc['document_text'] for doc in documents if len(doc['document_text'])>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') + list(string.punctuation)\n",
    "if not load_from_pickle:\n",
    "    document_model = DocumentModels(wiki_en_model.get_model().wv, document_texts, stop_words)\n",
    "    document_model.embed_documents(use_tfidf_weighing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_from_pickle:\n",
    "    embedding = document_model.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not load_from_pickle:\n",
    "#     u, s, vh = np.linalg.svd(embedding, full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_singular_vector = vh[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not load_from_pickle:\n",
    "#     for i in range(len(document_texts)):\n",
    "#         embedding[i, :] = embedding[i, :] - first_singular_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarity Analysis\n",
    "\n",
    "In order to do analysis on a corpus of documents we will use module 'DocumentSimilarity'. Below are some examples of use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_from_pickle:\n",
    "    document_embedding = document_model.get_embedding()\n",
    "    ds = DocumentSimilarity(document_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3100564"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.euclid_similarity(document_embedding[0], document_embedding[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_from_pickle:\n",
    "    list_of_neighbors = ds.k_nearest_neighbors(document_embedding[43877])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43877, 44089, 41353, 42629, 46425, 47842, 53465, 34301, 51042, 32548]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written Question No 945/88 by Mr Luc Beyer de Ryke to the Commission: Gas imports\n",
      " \n",
      "Written Question No 1613/88 by Mr Bryan Cassidy to the Commission: Distillation into alcohol of surplus apples\n",
      " \n",
      "WRITTEN QUESTION NO 2564/86 BY MR BARRY SEAL TO THE COMMISSION: GENERAL AVIATION LICENCES\n",
      " \n",
      "WRITTEN QUESTION NO 1482/87 BY MR POL MARCK TO THE COMMISSION: PORTUGUESE CEREAL IMPORTS\n",
      " \n",
      "WRITTEN QUESTION No. 2649/90 by Mr Mark KILLILEA to the Commission. Radon gas levels in the west of Ireland\n",
      " \n",
      "WRITTEN QUESTION No. 1966/91 by Mr Wilfried TELKÃ„MPER to the Commission. Transposition into German law of the EIA directive\n",
      " \n",
      "QUESTION No 80 (H-0467/94) by Petrus CORNELISSEN to the Commission. Transport of animals\n",
      " \n",
      "WRITTEN QUESTION NO 219/81 BY MR PROVAN TO THE COMMISSION: TAXATION DIFFERENCES AFFECTING AGRICULTURE AND FORESTRY\n",
      " \n",
      "WRITTEN QUESTION No. 1154/93 by Paul STAES to the Council. The ozone layer\n",
      " \n",
      "QUESTION ECRITE NO 208/79 DE M. ANSQUER A LA COMMISSION: PRIX DES PRODUITS PETROLIERS\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i in list_of_neighbors:\n",
    "    print(documents[i]['document_title'])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_to_pickle = True\n",
    "if dump_to_pickle:\n",
    "    with open(\"document_analysis_variables.pkl\", \"wb\") as f:\n",
    "        pickle.dump([documents, sample_model, document_texts, document_model, embedding, u, s, vh, document_embedding, ds, \n",
    "                     list_of_neighbors], f, protocol=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
